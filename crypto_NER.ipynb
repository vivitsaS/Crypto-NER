{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d842e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c52bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets to paths\n",
    "\n",
    "path = 'C:\\\\Users\\\\vivit\\\\Downloads\\\\data.csv'\n",
    "path2 = 'C:\\\\Users\\\\vivit\\\\Downloads\\\\term_def.csv'\n",
    "path3 = 'C:\\\\Users\\\\vivit\\\\Downloads\\\\term_abb.csv'\n",
    "\n",
    "\n",
    "# import datasets into dataframes\n",
    "\n",
    "data_df = read_csv(path)\n",
    "term_def_df = read_csv(path2)\n",
    "term_abb_df = read_csv(path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b76d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321712</td>\n",
       "      <td>Hey ðŸ‘‹ \\n\\nWe re using our bot:\\n\\nhttps://t.me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321713</td>\n",
       "      <td>Good stuff \\n\\nI am surprised I took so long t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321717</td>\n",
       "      <td>you are using a non-official one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321718</td>\n",
       "      <td>use the one that uniswap uses: https://thegrap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321719</td>\n",
       "      <td>keep in mind this is a hot subgraph so it can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44131</th>\n",
       "      <td>374466</td>\n",
       "      <td>Can find it in many places\\nAlso on Santiment:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44132</th>\n",
       "      <td>374467</td>\n",
       "      <td>guys, does anyone know if there is an applicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44133</th>\n",
       "      <td>374468</td>\n",
       "      <td>Any Lobsters going to Kyiv Web3 Hackathon Sept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44134</th>\n",
       "      <td>374469</td>\n",
       "      <td>whats funny is that no one complains about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44135</th>\n",
       "      <td>374470</td>\n",
       "      <td>By the way, which do you think is the on-chain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44136 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            content\n",
       "0      321712  Hey ðŸ‘‹ \\n\\nWe re using our bot:\\n\\nhttps://t.me...\n",
       "1      321713  Good stuff \\n\\nI am surprised I took so long t...\n",
       "2      321717                   you are using a non-official one\n",
       "3      321718  use the one that uniswap uses: https://thegrap...\n",
       "4      321719  keep in mind this is a hot subgraph so it can ...\n",
       "...       ...                                                ...\n",
       "44131  374466  Can find it in many places\\nAlso on Santiment:...\n",
       "44132  374467  guys, does anyone know if there is an applicat...\n",
       "44133  374468  Any Lobsters going to Kyiv Web3 Hackathon Sept...\n",
       "44134  374469  whats funny is that no one complains about the...\n",
       "44135  374470  By the way, which do you think is the on-chain...\n",
       "\n",
       "[44136 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7afdcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract keywords/phrases from a given sentence\n",
    "\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from collections import Counter\n",
    "  \n",
    "import spacy \n",
    "\"\"\"nltk.download('punkt')\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "\n",
    "import re, string\n",
    "\n",
    "import spacy \n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n",
    "all_stop_words = sp.Defaults.stop_words\n",
    "\n",
    "from rake_nltk import Rake\n",
    "r = Rake(stopwords = all_stop_words,max_length = 3)\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "vectorizer = KeyphraseCountVectorizer()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model=sentence_model)\n",
    "\n",
    "def clean(sentence):\n",
    "\n",
    "    # remove all characters that are not alphanumeric characters (/w) and white space and tab (\\s) \n",
    "    # and replace them with \"\"\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "\n",
    "    # remove links\n",
    "    sentence = re.sub(r'http\\S+', '',sentence)\n",
    "    # lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # remove punctuations\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "\n",
    "    return sentence\n",
    "    \n",
    "\n",
    "def get_article_text_list(article_url):\n",
    "    \n",
    "    scrapped_data = urllib.request.urlopen(article_url)\n",
    "    article = scrapped_data .read()\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    article_text_list = []\n",
    "    for p in paragraphs:  \n",
    "        article_text_list.append(clean(p.text))\n",
    "            \n",
    "    return article_text_list\n",
    "    \n",
    "def get_keyW_from_long_text(sentence_list):\n",
    "\n",
    "\n",
    "    # Init KeyBERT\n",
    "\n",
    "    # here 'keyphrases' gives a list of list of keywords\n",
    "    keyphrases = kw_model.extract_keywords(docs=sentence_list, stop_words = 'english', \n",
    "                                            use_mmr = \"True\", diversity = 0.8, top_n = 2)\n",
    "\n",
    "    \n",
    "    keywords_list = []\n",
    "    for keyphrase_list in keyphrases:\n",
    "        for keyphrase in keyphrase_list:\n",
    "            if keyphrase not in keywords_list:\n",
    "                keywords_list.append(keyphrase[0])\n",
    "\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "def get_most_common_words(word_list, number_of_words_to_be_extracted):\n",
    "    # Pass the split_it list to instance of Counter class.\n",
    "    \n",
    "    for word in word_list:\n",
    "        counter = Counter(word_list)\n",
    "        most_occur = counter.most_common(number_of_words_to_be_extracted)\n",
    "        m_o_list = []\n",
    "    for m_o in most_occur:\n",
    "        m_o_list.append(m_o[0])\n",
    "        \n",
    "    return m_o_list\n",
    "    \n",
    "def get_keyW_from_short_text(method,sentence):\n",
    "    \n",
    "    if method == \"rake\":\n",
    "\n",
    "        r.extract_keywords_from_text(sentence)\n",
    "        res = r.get_ranked_phrases()[0:3]\n",
    "        return res\n",
    "    \n",
    "    elif method == \"spacy_nouns\":\n",
    "        \n",
    "        sentence = sp(sentence)\n",
    "        res = sentence.noun_chunks\n",
    "        keyW_str = []\n",
    "        for word in res:\n",
    "            word_tokens = TreebankWordTokenizer().tokenize(word.text)\n",
    "            # removes stopwords from noun phrases \n",
    "            filtered_phrase = [w for w in word_tokens if not w in all_stop_words]\n",
    "            filtered_phrase = TreebankWordDetokenizer().detokenize(filtered_phrase)\n",
    "            if filtered_phrase!='':\n",
    "                keyW_str.append(filtered_phrase)\n",
    "        \n",
    "        return keyW_str\n",
    "        \n",
    "    else: \n",
    "        print(method + \" not recognised as a method! Try 'spacy_nouns' or 'rake'\")\n",
    "        return None\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "# function to build vocabulary list\n",
    "def build_vocab(vocab_list , new_words_list):\n",
    "    flag = 0\n",
    "    for new_word in new_words_list:\n",
    "\n",
    "        if new_word not in vocab_list:\n",
    "            vocab_list.append(new_word)  \n",
    "            \n",
    "    vocab_list.sort()\n",
    "    return vocab_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551d0731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# extracting keywords form sentences and importing them into another column in the dataframe\\nfor i in range(len(data_df['content'])):\\n    data_df['keywords'][i] = get_keyW_from_short_text(clean(data_df['content'][i]))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove nan containing rows\n",
    "\n",
    "data_df = data_df.dropna()\n",
    "data_df = data_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "68bcb395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivit\\anaconda3\\envs\\StockAI37\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# clean the dataset\n",
    "\n",
    "for i in range(len(data_df)):\n",
    "    data_df['content'][i] = clean(data_df['content'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32c8d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>crytpo_related_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321712</td>\n",
       "      <td>hey  \\n\\nwe re using our bot\\n\\n\\n\\nand also f...</td>\n",
       "      <td>[topics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321713</td>\n",
       "      <td>good stuff \\n\\ni am surprised i took so long t...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321717</td>\n",
       "      <td>you are using a nonofficial one</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321718</td>\n",
       "      <td>use the one that uniswap uses</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321719</td>\n",
       "      <td>keep in mind this is a hot subgraph so it can ...</td>\n",
       "      <td>[hot subgraph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43481</th>\n",
       "      <td>374466</td>\n",
       "      <td>can find it in many places\\nalso on santiment ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43482</th>\n",
       "      <td>374467</td>\n",
       "      <td>guys does anyone know if there is an applicati...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43483</th>\n",
       "      <td>374468</td>\n",
       "      <td>any lobsters going to kyiv web3 hackathon sept...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43484</th>\n",
       "      <td>374469</td>\n",
       "      <td>whats funny is that no one complains about the...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43485</th>\n",
       "      <td>374470</td>\n",
       "      <td>by the way which do you think is the onchain a...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            content  \\\n",
       "0      321712  hey  \\n\\nwe re using our bot\\n\\n\\n\\nand also f...   \n",
       "1      321713  good stuff \\n\\ni am surprised i took so long t...   \n",
       "2      321717                    you are using a nonofficial one   \n",
       "3      321718                     use the one that uniswap uses    \n",
       "4      321719  keep in mind this is a hot subgraph so it can ...   \n",
       "...       ...                                                ...   \n",
       "43481  374466  can find it in many places\\nalso on santiment ...   \n",
       "43482  374467  guys does anyone know if there is an applicati...   \n",
       "43483  374468  any lobsters going to kyiv web3 hackathon sept...   \n",
       "43484  374469  whats funny is that no one complains about the...   \n",
       "43485  374470  by the way which do you think is the onchain a...   \n",
       "\n",
       "      crytpo_related_keywords  \n",
       "0                    [topics]  \n",
       "1                          []  \n",
       "2                          []  \n",
       "3                          []  \n",
       "4              [hot subgraph]  \n",
       "...                       ...  \n",
       "43481                          \n",
       "43482                          \n",
       "43483                          \n",
       "43484                          \n",
       "43485                          \n",
       "\n",
       "[43486 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a420694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocabulary from given data\n",
    "\n",
    "new_words_list_list1 = [term_def_df['terms'].tolist(),term_abb_df['terms'].tolist(), \n",
    "              term_abb_df['abbreviations'].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d1ecc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the above lists\n",
    "\n",
    "vocab = []\n",
    "for new_words_list_ in new_words_list_list1:\n",
    "    for word in new_words_list_:\n",
    "        vocab.append(clean(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc6d2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "63952325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ethereum.org/en/eth/\n",
      "73\n",
      "10\n",
      "https://www.geeksforgeeks.org/introduction-to-polygon/\n",
      "36\n",
      "10\n",
      "https://www.forbes.com/advisor/in/investing/cryptocurrency/best-cryptocurrency-exchanges/\n",
      "192\n",
      "10\n",
      "https://en.wikipedia.org/wiki/Web3\n",
      "36\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# extracting new words and adding to vocab. This pulls 10 most prominent word from a given article.\n",
    "\n",
    "article_ur1s = ['https://ethereum.org/en/eth/',\n",
    "                'https://www.geeksforgeeks.org/introduction-to-polygon/',\n",
    "               'https://www.forbes.com/advisor/in/investing/cryptocurrency/best-cryptocurrency-exchanges/',\n",
    "               'https://en.wikipedia.org/wiki/Web3'\n",
    "                ]\n",
    "\n",
    "for article_url in article_ur1s:\n",
    "    print(article_url)\n",
    "    x = get_keyW_from_long_text(get_article_text_list(article_url))\n",
    "    print(len(x))\n",
    "    x = get_most_common_words(x,10)\n",
    "    print(len(x))\n",
    "    vocab = build_vocab(vocab,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48a83899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 34 new words were found in the articles and added to the vocabulary\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9fa1b3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding some basic words manually\n",
    "\n",
    "new_words = ['ether', 'coin', 'swap', 'exchange','mine']\n",
    "build_vocab(vocab, new_words)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e6d7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to extract crypto related keywords using spacy's phrase matcher\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "phrase_matcher = PhraseMatcher(sp.vocab)\n",
    "\n",
    "patterns = [sp(text) for text in vocab]\n",
    "phrase_matcher.add('matcher', None, *patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3697c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can they just make a token already please \n",
      "token\n"
     ]
    }
   ],
   "source": [
    "# this matcher is able to detect the word 'token' present in the vocab\n",
    "\n",
    "sentence = sp(data_df['content'][93])\n",
    "print(sentence)\n",
    "matched_phrases = phrase_matcher(sentence)\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = sp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e72e3fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats funny is that no one complains about the txs being rejected and censored by ethermine and flashbots right now not related to ofac\n"
     ]
    }
   ],
   "source": [
    "# But it isn't able detect teh word 'ethermine', even though the words 'ether' and 'mine'\n",
    "# exist in the vocab\n",
    "\n",
    "sentence = sp(data_df['content'][43484])\n",
    "print(sentence)\n",
    "matched_phrases = phrase_matcher(sentence)\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = sp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7daa7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class contains embedder tools\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "class wordEmbedder:\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        \n",
    "        self.embedder = embedder\n",
    "\n",
    "        \n",
    "    def get_vectors(self, list_1, list_2):\n",
    "        \n",
    "        self.x = word_tokenize(list_1) \n",
    "        self.y = word_tokenize(list_2) \n",
    "        \n",
    "        if self.embedder == \"basic\":\n",
    "            x,y = self.basic_embedder()\n",
    "        elif self.embedder == \"word sensitive\":\n",
    "            x,y = self.word_sensitive_embedder()\n",
    "        else:\n",
    "            raise Exception(embedder+\" embedder not found!\")\n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "    \n",
    "    # utility functions\n",
    "    \n",
    "    # stemmer for list of words\n",
    "    def get_stemmed_list(self,list_):\n",
    "        stemmed_list = []\n",
    "        for word in list_:\n",
    "            stemmed_list.append(ps.stem(word))\n",
    "        return stemmed_list\n",
    "\n",
    "    # lemmatizer for list of words\n",
    "    def get_lemmatized_list(self,list_):\n",
    "        lemmatized_list = []\n",
    "        for word in list_:\n",
    "            lemmatized_list.append(ps.wnl(word))\n",
    "        return lemmatized_list\n",
    "\n",
    "    def is_substring(self,substring, fullstring):\n",
    "        is_substring = 0\n",
    "        if substring == '' or fullstring == '':\n",
    "            return is_substring\n",
    "        if substring in fullstring:\n",
    "            is_substring = 1\n",
    "        return is_substring\n",
    "\n",
    "            \n",
    "    # takes sentences as input\n",
    "    def basic_embedder(self):\n",
    "        \n",
    "        self.x = self.get_stemmed_list(self.x)\n",
    "        self.y = self.get_stemmed_list(self.y)\n",
    "        \n",
    "        counter1 = Counter(self.x)\n",
    "        counter2= Counter(self.y)\n",
    "    \n",
    "        all_items = set(counter1.keys()).union( set(counter2.keys()) )\n",
    "        vector1 = [counter1[k] for k in all_items]\n",
    "        vector2 = [counter2[k] for k in all_items]\n",
    "        vector1 = np.array(vector1)\n",
    "        vector2 = np.array(vector2)\n",
    "    \n",
    "        return vector1, vector2\n",
    "\n",
    "    \n",
    "    def word_sensitive_embedder(self):\n",
    "\n",
    "        # x = vocab word/phrase, y = key-word/phrase  \n",
    "        # if there is a word in y that contains a word in the voacb\n",
    "        # for ex, crypto-___, it needs to detected\n",
    "        \n",
    "        counter1 = Counter(self.x)\n",
    "        counter2 = Counter(self.y)\n",
    "\n",
    "        all_items = set(counter1.keys()).union( set(counter2.keys()) )\n",
    "\n",
    "        for x_ in self.x:\n",
    "            vector1 = [self.is_substring(x_,k) for k in all_items]\n",
    "        vector2 = [counter2[k] for k in all_items]\n",
    "        vector1 = np.array(vector1)\n",
    "        vector2 = np.array(vector2)\n",
    "\n",
    "        return vector1, vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dab059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class contains similarity detection tools\n",
    "\n",
    "class keyMatcher:\n",
    "\n",
    "    def __init__(self, matcher):\n",
    "        \n",
    "        self.matcher = matcher\n",
    "        \n",
    "        \n",
    "    def get_similarity(self, vector_1, vector_2):\n",
    "        \n",
    "        self.x = vector_1\n",
    "        self.y = vector_2\n",
    "        \n",
    "        if self.matcher == \"jaccard\":\n",
    "            similarity = self.jaccard_similarity()\n",
    "        elif self.matcher == \"cosine\":\n",
    "            similarity = self.cosine_similarity()\n",
    "        else:\n",
    "            raise Exception(matcher+\" matcher not found!\")\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def jaccard_similarity(self):\n",
    "    \n",
    "        intersection = np.logical_and(self.x, self.y)\n",
    "        union = np.logical_or(self.x, self.y)\n",
    "        jaccard_similarity = intersection.sum() / float(union.sum())\n",
    "        \n",
    "        return jaccard_similarity\n",
    "    \n",
    "    def cosine_similarity(self):\n",
    "        # Compute the dot product between x and y\n",
    "        dot_product = np.dot(self.x, self.y)\n",
    "\n",
    "        # Compute the L2 norms (magnitudes) of x and y\n",
    "        magnitude_x = np.sqrt(np.sum(self.x**2)) \n",
    "        magnitude_y = np.sqrt(np.sum(self.y**2))\n",
    "\n",
    "        # Compute the cosine similarity\n",
    "        cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
    "\n",
    "        return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d17dd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision function to extract crypto related words. It checks for a match between words from the\n",
    "# vocab and inputted sentence\n",
    "\n",
    "def get_crypto_related_keys(threshold,keyword_extraction_method,embed_func, sim_func, vocab,sentence):\n",
    "\n",
    "    crypto_related_keys = []\n",
    "    keyW_list = get_keyW_from_short_text(keyword_extraction_method,sentence)\n",
    "    for keyW in keyW_list:\n",
    "        sim = 0\n",
    "        for word in vocab:\n",
    "            x,y = embed_func(word,keyW)\n",
    "            sim += sim_func(x,y)\n",
    "        if sim>=threshold:\n",
    "            crypto_related_keys.append(keyW)\n",
    "  \n",
    "    return crypto_related_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "abb59363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivit\\anaconda3\\envs\\StockAI37\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "embed_func = wordEmbedder(\"word sensitive\").get_vectors\n",
    "sim_func = keyMatcher(\"cosine\").get_similarity\n",
    "data_df['crytpo_related_keywords'] = ''\n",
    "for i in range(len(data_df)):\n",
    "    data_df['crytpo_related_keywords'][i] = get_crypto_related_keys(threshold,'spacy_nouns',embed_func,sim_func,\n",
    "                                                                vocab,data_df['content'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63bf182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>crytpo_related_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>371624</td>\n",
       "      <td>the catholic church didnt think that all books...</td>\n",
       "      <td>[catholic church]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>371625</td>\n",
       "      <td>clickbait title going to clickbait</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>371626</td>\n",
       "      <td>not following</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>371627</td>\n",
       "      <td>now your denzinger too</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41144</th>\n",
       "      <td>371628</td>\n",
       "      <td>the 95 theses were written by martin luther</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43481</th>\n",
       "      <td>374466</td>\n",
       "      <td>can find it in many places\\nalso on santiment ...</td>\n",
       "      <td>[santiment historical balances, selected time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43482</th>\n",
       "      <td>374467</td>\n",
       "      <td>guys does anyone know if there is an applicati...</td>\n",
       "      <td>[wallets, connection, transactions, wallets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43483</th>\n",
       "      <td>374468</td>\n",
       "      <td>any lobsters going to kyiv web3 hackathon sept...</td>\n",
       "      <td>[kyiv web3 hackathon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43484</th>\n",
       "      <td>374469</td>\n",
       "      <td>whats funny is that no one complains about the...</td>\n",
       "      <td>[ethermine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43485</th>\n",
       "      <td>374470</td>\n",
       "      <td>by the way which do you think is the onchain a...</td>\n",
       "      <td>[onchain analytics]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2346 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            content  \\\n",
       "41140  371624  the catholic church didnt think that all books...   \n",
       "41141  371625                 clickbait title going to clickbait   \n",
       "41142  371626                                      not following   \n",
       "41143  371627                             now your denzinger too   \n",
       "41144  371628        the 95 theses were written by martin luther   \n",
       "...       ...                                                ...   \n",
       "43481  374466  can find it in many places\\nalso on santiment ...   \n",
       "43482  374467  guys does anyone know if there is an applicati...   \n",
       "43483  374468  any lobsters going to kyiv web3 hackathon sept...   \n",
       "43484  374469  whats funny is that no one complains about the...   \n",
       "43485  374470  by the way which do you think is the onchain a...   \n",
       "\n",
       "                                 crytpo_related_keywords  \n",
       "41140                                  [catholic church]  \n",
       "41141                                                 []  \n",
       "41142                                                 []  \n",
       "41143                                                 []  \n",
       "41144                                                 []  \n",
       "...                                                  ...  \n",
       "43481  [santiment historical balances, selected time ...  \n",
       "43482       [wallets, connection, transactions, wallets]  \n",
       "43483                              [kyiv web3 hackathon]  \n",
       "43484                                        [ethermine]  \n",
       "43485                                [onchain analytics]  \n",
       "\n",
       "[2346 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[41140:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d1baf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('C:\\\\Users\\\\vivit\\\\Downloads\\\\data_df_w_crypto_keyW1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "be4bd523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analyze blockchain security']\n",
      "['onchain analytics']\n",
      "['tokensoft today', 'polygon']\n"
     ]
    }
   ],
   "source": [
    "# test example\n",
    "threshold = 0.7\n",
    "s1 = \"to analyze blockchain security and find even better selfish mining techniques\"\n",
    "s2 =  \"By the way, which do you think is the onchain analytics with the best user experience? Preferably the onesthat are self served and that everyone on the team can use\"\n",
    "s3 = \"We're launching incentivized testnet on polygon at tokensoft today\"\n",
    "s_list = [s1,s2,s3]\n",
    "embed_func = wordEmbedder(\"word sensitive\").get_vectors\n",
    "sim_func = keyMatcher(\"cosine\").get_similarity\n",
    "\n",
    "for s in s_list:\n",
    "    s = clean(s)\n",
    "    print(get_crypto_related_keys(threshold,'rake',embed_func,sim_func, vocab, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ee516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Advantages of this method:\n",
    "* model-free\n",
    "* will work for any given corpus\n",
    "* works on a very small set of vocab words as it is purely algorithm based\n",
    "* the embeddings are designed such that words that contain those occuring in the vocab as substrings\n",
    "are also detected. For example, it is enough to have the word 'crypto' in order to detect words like \n",
    "'cryptoeconomics'. Or 'ether' to detect the word 'ethermine'.\n",
    "Limitations:\n",
    "* the embeddings don't work well when the keywords/phrases extracted are too long\n",
    "* even though it doesn't use a model, the time taken is quite high. Not very space/time efficient.\n",
    "* these limitations can be overcome by pruning the vocabulary thoroughly.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
